#![forbid(unsafe_code)]

use fj_core::{DType, Literal, Primitive, Shape, TensorValue, Value};
use std::collections::BTreeMap;

use crate::EvalError;
use crate::type_promotion::{binary_literal_op, promote_dtype};

/// Binary elementwise operation dispatching on int/float paths.
/// Supports full NumPy broadcasting: scalar-scalar, tensor-tensor (same shape),
/// scalar-tensor, tensor-scalar, and multi-dim broadcasting.
#[inline]
pub(crate) fn eval_binary_elementwise(
    primitive: Primitive,
    inputs: &[Value],
    int_op: impl Fn(i64, i64) -> i64,
    float_op: impl Fn(f64, f64) -> f64,
) -> Result<Value, EvalError> {
    if inputs.len() != 2 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2,
            actual: inputs.len(),
        });
    }

    match (&inputs[0], &inputs[1]) {
        (Value::Scalar(lhs), Value::Scalar(rhs)) => Ok(Value::Scalar(binary_literal_op(
            *lhs, *rhs, primitive, &int_op, &float_op,
        )?)),
        (Value::Tensor(lhs), Value::Tensor(rhs)) => {
            if lhs.shape == rhs.shape {
                // Same shape: elementwise
                let elements = lhs
                    .elements
                    .iter()
                    .copied()
                    .zip(rhs.elements.iter().copied())
                    .map(|(left, right)| {
                        binary_literal_op(left, right, primitive, &int_op, &float_op)
                    })
                    .collect::<Result<Vec<_>, _>>()?;

                let dtype = promote_dtype(lhs.dtype, rhs.dtype);
                Ok(Value::Tensor(TensorValue::new(
                    dtype,
                    lhs.shape.clone(),
                    elements,
                )?))
            } else {
                // Attempt NumPy multi-dim broadcasting
                broadcast_binary_tensors(primitive, lhs, rhs, &int_op, &float_op)
            }
        }
        (Value::Scalar(lhs), Value::Tensor(rhs)) => {
            let elements = rhs
                .elements
                .iter()
                .copied()
                .map(|right| binary_literal_op(*lhs, right, primitive, &int_op, &float_op))
                .collect::<Result<Vec<_>, _>>()?;

            let lhs_dtype = match lhs {
                Literal::I64(_) => DType::I64,
                Literal::F64Bits(_) => DType::F64,
                Literal::Bool(_) => DType::Bool,
                Literal::Complex64Bits(..) => DType::Complex64,
                Literal::Complex128Bits(..) => DType::Complex128,
            };
            let dtype = promote_dtype(lhs_dtype, rhs.dtype);
            Ok(Value::Tensor(TensorValue::new(
                dtype,
                rhs.shape.clone(),
                elements,
            )?))
        }
        (Value::Tensor(lhs), Value::Scalar(rhs)) => {
            let elements = lhs
                .elements
                .iter()
                .copied()
                .map(|left| binary_literal_op(left, *rhs, primitive, &int_op, &float_op))
                .collect::<Result<Vec<_>, _>>()?;

            let rhs_dtype = match rhs {
                Literal::I64(_) => DType::I64,
                Literal::F64Bits(_) => DType::F64,
                Literal::Bool(_) => DType::Bool,
                Literal::Complex64Bits(..) => DType::Complex64,
                Literal::Complex128Bits(..) => DType::Complex128,
            };
            let dtype = promote_dtype(lhs.dtype, rhs_dtype);
            Ok(Value::Tensor(TensorValue::new(
                dtype,
                lhs.shape.clone(),
                elements,
            )?))
        }
    }
}

/// Full NumPy multi-dim broadcasting for two tensors.
fn broadcast_binary_tensors(
    primitive: Primitive,
    lhs: &TensorValue,
    rhs: &TensorValue,
    int_op: &impl Fn(i64, i64) -> i64,
    float_op: &impl Fn(f64, f64) -> f64,
) -> Result<Value, EvalError> {
    let out_shape = broadcast_shape(&lhs.shape, &rhs.shape).ok_or(EvalError::ShapeMismatch {
        primitive,
        left: lhs.shape.clone(),
        right: rhs.shape.clone(),
    })?;

    let out_count = out_shape.element_count().unwrap_or(0) as usize;
    let out_strides = compute_strides(&out_shape.dims);

    // Compute broadcast-aware strides for lhs and rhs
    let lhs_strides = broadcast_strides(&lhs.shape, &out_shape);
    let rhs_strides = broadcast_strides(&rhs.shape, &out_shape);

    let mut elements = Vec::with_capacity(out_count);
    for flat_idx in 0..out_count {
        // Convert output flat index to multi-index
        let multi = flat_to_multi(flat_idx, &out_strides);
        // Map to input indices
        let lhs_idx = broadcast_flat_index(&multi, &lhs_strides);
        let rhs_idx = broadcast_flat_index(&multi, &rhs_strides);

        let l = lhs.elements[lhs_idx];
        let r = rhs.elements[rhs_idx];
        elements.push(binary_literal_op(l, r, primitive, int_op, float_op)?);
    }

    let dtype = promote_dtype(lhs.dtype, rhs.dtype);
    Ok(Value::Tensor(TensorValue::new(dtype, out_shape, elements)?))
}

fn broadcast_shape(lhs: &Shape, rhs: &Shape) -> Option<Shape> {
    let max_rank = lhs.rank().max(rhs.rank());
    let mut dims = Vec::with_capacity(max_rank);

    for offset in 0..max_rank {
        let lhs_dim = if offset < lhs.rank() {
            lhs.dims[lhs.rank() - 1 - offset]
        } else {
            1
        };
        let rhs_dim = if offset < rhs.rank() {
            rhs.dims[rhs.rank() - 1 - offset]
        } else {
            1
        };

        let out_dim = if lhs_dim == rhs_dim {
            lhs_dim
        } else if lhs_dim == 1 {
            rhs_dim
        } else if rhs_dim == 1 {
            lhs_dim
        } else {
            return None;
        };
        dims.push(out_dim);
    }

    dims.reverse();
    Some(Shape { dims })
}

fn compute_strides(dims: &[u32]) -> Vec<usize> {
    let mut strides = vec![1_usize; dims.len()];
    for i in (0..dims.len().saturating_sub(1)).rev() {
        strides[i] = strides[i + 1] * dims[i + 1] as usize;
    }
    strides
}

fn flat_to_multi(flat: usize, strides: &[usize]) -> Vec<usize> {
    let mut multi = Vec::with_capacity(strides.len());
    let mut remainder = flat;
    for &stride in strides {
        multi.push(remainder / stride);
        remainder %= stride;
    }
    multi
}

/// Compute strides for a tensor being broadcast to out_shape.
/// Dimensions of size 1 get stride 0 (broadcast), left-padded with 0s.
fn broadcast_strides(shape: &Shape, out_shape: &Shape) -> Vec<usize> {
    let rank = shape.rank();
    let out_rank = out_shape.rank();

    // Compute real strides for the input tensor
    let real_strides = compute_strides(&shape.dims);

    let mut result = vec![0_usize; out_rank];
    for (i, &stride) in real_strides.iter().enumerate().take(rank) {
        let out_axis = out_rank - rank + i;
        if shape.dims[i] == 1 {
            result[out_axis] = 0; // broadcast
        } else {
            result[out_axis] = stride;
        }
    }
    result
}

fn broadcast_flat_index(multi: &[usize], strides: &[usize]) -> usize {
    multi.iter().zip(strides.iter()).map(|(&m, &s)| m * s).sum()
}

/// Unary elementwise operation that converts to f64 first (exp, log, sqrt, etc.).
#[inline]
pub(crate) fn eval_unary_elementwise(
    primitive: Primitive,
    inputs: &[Value],
    op: impl Fn(f64) -> f64,
) -> Result<Value, EvalError> {
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    match &inputs[0] {
        Value::Scalar(literal) => {
            let value = literal.as_f64().ok_or(EvalError::TypeMismatch {
                primitive,
                detail: "expected numeric scalar",
            })?;
            Ok(Value::scalar_f64(op(value)))
        }
        Value::Tensor(tensor) => {
            let elements = tensor
                .elements
                .iter()
                .copied()
                .map(|literal| {
                    literal.as_f64().map(&op).map(Literal::from_f64).ok_or(
                        EvalError::TypeMismatch {
                            primitive,
                            detail: "expected numeric tensor elements",
                        },
                    )
                })
                .collect::<Result<Vec<_>, _>>()?;

            Ok(Value::Tensor(TensorValue::new(
                DType::F64,
                tensor.shape.clone(),
                elements,
            )?))
        }
    }
}

/// Unary elementwise that preserves integer types (for neg, abs).
#[inline]
pub(crate) fn eval_unary_int_or_float(
    primitive: Primitive,
    inputs: &[Value],
    int_op: impl Fn(i64) -> i64,
    float_op: impl Fn(f64) -> f64,
) -> Result<Value, EvalError> {
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    match &inputs[0] {
        Value::Scalar(literal) => match *literal {
            Literal::I64(v) => Ok(Value::scalar_i64(int_op(v))),
            Literal::F64Bits(bits) => Ok(Value::scalar_f64(float_op(f64::from_bits(bits)))),
            Literal::Bool(_) => Err(EvalError::TypeMismatch {
                primitive,
                detail: "expected numeric scalar, got bool",
            }),
            Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                Err(EvalError::TypeMismatch {
                    primitive,
                    detail: "complex arithmetic not yet implemented",
                })
            }
        },
        Value::Tensor(tensor) => {
            let elements = tensor
                .elements
                .iter()
                .copied()
                .map(|literal| match literal {
                    Literal::I64(v) => Ok(Literal::I64(int_op(v))),
                    Literal::F64Bits(bits) => Ok(Literal::from_f64(float_op(f64::from_bits(bits)))),
                    Literal::Bool(_) => Err(EvalError::TypeMismatch {
                        primitive,
                        detail: "expected numeric tensor elements, got bool",
                    }),
                    Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                        Err(EvalError::TypeMismatch {
                            primitive,
                            detail: "complex arithmetic not yet implemented",
                        })
                    }
                })
                .collect::<Result<Vec<_>, _>>()?;

            let dtype = tensor.dtype;
            Ok(Value::Tensor(TensorValue::new(
                dtype,
                tensor.shape.clone(),
                elements,
            )?))
        }
    }
}

/// Select operation: select(cond, on_true, on_false) -> on_true where cond else on_false.
pub(crate) fn eval_select(primitive: Primitive, inputs: &[Value]) -> Result<Value, EvalError> {
    if inputs.len() != 3 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 3,
            actual: inputs.len(),
        });
    }

    match (&inputs[0], &inputs[1], &inputs[2]) {
        (Value::Scalar(cond), Value::Scalar(on_true), Value::Scalar(on_false)) => {
            let c = match cond {
                Literal::Bool(b) => *b,
                Literal::I64(v) => *v != 0,
                Literal::F64Bits(bits) => f64::from_bits(*bits) != 0.0,
                Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                    return Err(EvalError::TypeMismatch {
                        primitive,
                        detail: "complex condition not yet supported for select",
                    });
                }
            };
            let val = if c { *on_true } else { *on_false };
            let lhs_dtype = match on_true {
                Literal::I64(_) => DType::I64,
                Literal::F64Bits(_) => DType::F64,
                Literal::Bool(_) => DType::Bool,
                Literal::Complex64Bits(..) => DType::Complex64,
                Literal::Complex128Bits(..) => DType::Complex128,
            };
            let rhs_dtype = match on_false {
                Literal::I64(_) => DType::I64,
                Literal::F64Bits(_) => DType::F64,
                Literal::Bool(_) => DType::Bool,
                Literal::Complex64Bits(..) => DType::Complex64,
                Literal::Complex128Bits(..) => DType::Complex128,
            };
            let dtype = promote_dtype(lhs_dtype, rhs_dtype);
            let promoted_val = match dtype {
                DType::F64 | DType::F32 => {
                    let f_val = val.as_f64().ok_or(EvalError::TypeMismatch {
                        primitive,
                        detail: "expected numeric scalar for select",
                    })?;
                    Literal::from_f64(f_val)
                }
                DType::I64 | DType::I32 => {
                    let i_val = val.as_i64().ok_or(EvalError::TypeMismatch {
                        primitive,
                        detail: "expected integer scalar for select",
                    })?;
                    Literal::I64(i_val)
                }
                DType::Bool => val,
                DType::Complex64 | DType::Complex128 => {
                    return Err(EvalError::TypeMismatch {
                        primitive,
                        detail: "complex arithmetic not yet implemented",
                    });
                }
            };
            Ok(Value::Scalar(promoted_val))
        }
        (Value::Tensor(cond), Value::Tensor(on_true), Value::Tensor(on_false)) => {
            if cond.shape != on_true.shape || cond.shape != on_false.shape {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: "select requires all inputs to have the same shape".to_owned(),
                });
            }
            let dtype = promote_dtype(on_true.dtype, on_false.dtype);
            let elements: Result<Vec<Literal>, EvalError> = cond
                .elements
                .iter()
                .zip(on_true.elements.iter())
                .zip(on_false.elements.iter())
                .map(|((c, t), f)| {
                    let flag = match c {
                        Literal::Bool(b) => *b,
                        Literal::I64(v) => *v != 0,
                        Literal::F64Bits(bits) => f64::from_bits(*bits) != 0.0,
                        Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                            return Err(EvalError::TypeMismatch {
                                primitive,
                                detail: "complex condition not yet supported for select",
                            });
                        }
                    };
                    let val = if flag { *t } else { *f };
                    match dtype {
                        DType::F64 | DType::F32 => {
                            let f_val = val.as_f64().ok_or(EvalError::TypeMismatch {
                                primitive,
                                detail: "expected numeric tensor elements for select",
                            })?;
                            Ok(Literal::from_f64(f_val))
                        }
                        DType::I64 | DType::I32 => {
                            let i_val = val.as_i64().ok_or(EvalError::TypeMismatch {
                                primitive,
                                detail: "expected integer tensor elements for select",
                            })?;
                            Ok(Literal::I64(i_val))
                        }
                        DType::Bool => Ok(val),
                        DType::Complex64 | DType::Complex128 => Err(EvalError::TypeMismatch {
                            primitive,
                            detail: "complex arithmetic not yet implemented",
                        }),
                    }
                })
                .collect();
            Ok(Value::Tensor(TensorValue::new(
                dtype,
                cond.shape.clone(),
                elements?,
            )?))
        }
        // Tensor cond + scalar on_true + scalar on_false: broadcast scalars
        (Value::Tensor(cond), Value::Scalar(on_true), Value::Scalar(on_false)) => {
            let dtype = promote_dtype(
                match on_true {
                    Literal::I64(_) => DType::I64,
                    Literal::F64Bits(_) => DType::F64,
                    Literal::Bool(_) => DType::Bool,
                    Literal::Complex64Bits(..) => DType::Complex64,
                    Literal::Complex128Bits(..) => DType::Complex128,
                },
                match on_false {
                    Literal::I64(_) => DType::I64,
                    Literal::F64Bits(_) => DType::F64,
                    Literal::Bool(_) => DType::Bool,
                    Literal::Complex64Bits(..) => DType::Complex64,
                    Literal::Complex128Bits(..) => DType::Complex128,
                },
            );
            let elements: Result<Vec<Literal>, EvalError> = cond
                .elements
                .iter()
                .map(|c| {
                    let flag = match c {
                        Literal::Bool(b) => *b,
                        Literal::I64(v) => *v != 0,
                        Literal::F64Bits(bits) => f64::from_bits(*bits) != 0.0,
                        Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                            return Err(EvalError::TypeMismatch {
                                primitive,
                                detail: "complex condition not yet supported for select",
                            });
                        }
                    };
                    let val = if flag { *on_true } else { *on_false };
                    match dtype {
                        DType::F64 | DType::F32 => {
                            let f_val = val.as_f64().ok_or(EvalError::TypeMismatch {
                                primitive,
                                detail: "expected numeric scalar for select",
                            })?;
                            Ok(Literal::from_f64(f_val))
                        }
                        DType::I64 | DType::I32 => {
                            let i_val = val.as_i64().ok_or(EvalError::TypeMismatch {
                                primitive,
                                detail: "expected integer scalar for select",
                            })?;
                            Ok(Literal::I64(i_val))
                        }
                        DType::Bool => Ok(val),
                        DType::Complex64 | DType::Complex128 => Err(EvalError::TypeMismatch {
                            primitive,
                            detail: "complex arithmetic not yet implemented",
                        }),
                    }
                })
                .collect();
            Ok(Value::Tensor(TensorValue::new(
                dtype,
                cond.shape.clone(),
                elements?,
            )?))
        }
        // Scalar cond + tensor on_true + tensor on_false
        (Value::Scalar(cond), Value::Tensor(on_true), Value::Tensor(on_false)) => {
            if on_true.shape != on_false.shape {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: "select requires on_true and on_false to have the same shape"
                        .to_owned(),
                });
            }
            let flag = match cond {
                Literal::Bool(b) => *b,
                Literal::I64(v) => *v != 0,
                Literal::F64Bits(bits) => f64::from_bits(*bits) != 0.0,
                Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                    return Err(EvalError::TypeMismatch {
                        primitive,
                        detail: "complex condition not yet supported for select",
                    });
                }
            };
            if flag {
                Ok(Value::Tensor(on_true.clone()))
            } else {
                Ok(Value::Tensor(on_false.clone()))
            }
        }
        _ => Err(EvalError::Unsupported {
            primitive,
            detail: "select requires matching scalar/tensor kinds".to_owned(),
        }),
    }
}

/// Clamp: clamp(x, lo, hi) = min(max(x, lo), hi).
/// Supports scalar and tensor inputs with broadcasting.
pub(crate) fn eval_clamp(primitive: Primitive, inputs: &[Value]) -> Result<Value, EvalError> {
    if inputs.len() != 3 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 3,
            actual: inputs.len(),
        });
    }

    fn clamp_literal(x: Literal, lo: Literal, hi: Literal) -> Result<Literal, &'static str> {
        match (x, lo, hi) {
            (Literal::I64(xv), Literal::I64(lov), Literal::I64(hiv)) => {
                Ok(Literal::I64(xv.max(lov).min(hiv)))
            }
            (Literal::F64Bits(xb), Literal::F64Bits(lob), Literal::F64Bits(hib)) => {
                let xf = f64::from_bits(xb);
                let lof = f64::from_bits(lob);
                let hif = f64::from_bits(hib);
                Ok(Literal::from_f64(xf.max(lof).min(hif)))
            }
            _ => {
                // Mixed types: promote to f64
                let xf = match x {
                    Literal::I64(v) => v as f64,
                    Literal::F64Bits(b) => f64::from_bits(b),
                    Literal::Bool(_) => return Err("clamp does not support bool"),
                    Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                        return Err("complex arithmetic not yet implemented");
                    }
                };
                let lof = match lo {
                    Literal::I64(v) => v as f64,
                    Literal::F64Bits(b) => f64::from_bits(b),
                    Literal::Bool(_) => return Err("clamp does not support bool"),
                    Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                        return Err("complex arithmetic not yet implemented");
                    }
                };
                let hif = match hi {
                    Literal::I64(v) => v as f64,
                    Literal::F64Bits(b) => f64::from_bits(b),
                    Literal::Bool(_) => return Err("clamp does not support bool"),
                    Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                        return Err("complex arithmetic not yet implemented");
                    }
                };
                Ok(Literal::from_f64(xf.max(lof).min(hif)))
            }
        }
    }

    match (&inputs[0], &inputs[1], &inputs[2]) {
        (Value::Scalar(x), Value::Scalar(lo), Value::Scalar(hi)) => {
            let result = clamp_literal(*x, *lo, *hi)
                .map_err(|detail| EvalError::TypeMismatch { primitive, detail })?;
            Ok(Value::Scalar(result))
        }
        (Value::Tensor(x), Value::Scalar(lo), Value::Scalar(hi)) => {
            let elements: Result<Vec<Literal>, EvalError> = x
                .elements
                .iter()
                .map(|elem| {
                    clamp_literal(*elem, *lo, *hi)
                        .map_err(|detail| EvalError::TypeMismatch { primitive, detail })
                })
                .collect();
            Ok(Value::Tensor(TensorValue::new(
                x.dtype,
                x.shape.clone(),
                elements?,
            )?))
        }
        (Value::Tensor(x), Value::Tensor(lo), Value::Tensor(hi)) => {
            if x.shape != lo.shape || x.shape != hi.shape {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: "clamp requires all tensor inputs to have the same shape".to_owned(),
                });
            }
            let elements: Result<Vec<Literal>, EvalError> = x
                .elements
                .iter()
                .zip(lo.elements.iter())
                .zip(hi.elements.iter())
                .map(|((xv, lov), hiv)| {
                    clamp_literal(*xv, *lov, *hiv)
                        .map_err(|detail| EvalError::TypeMismatch { primitive, detail })
                })
                .collect();
            Ok(Value::Tensor(TensorValue::new(
                x.dtype,
                x.shape.clone(),
                elements?,
            )?))
        }
        _ => Err(EvalError::Unsupported {
            primitive,
            detail: "clamp requires (tensor, scalar, scalar) or (tensor, tensor, tensor)"
                .to_owned(),
        }),
    }
}

/// Approximate erf using Abramowitz & Stegun formula (max error ~1.5e-7).
pub(crate) fn erf_approx(x: f64) -> f64 {
    if x == 0.0 {
        return x;
    }
    let sign = x.signum();
    let x = x.abs();
    let t = 1.0 / (1.0 + 0.3275911 * x);
    let poly = t
        * (0.254829592
            + t * (-0.284496736 + t * (1.421413741 + t * (-1.453152027 + t * 1.061405429))));
    sign * (1.0 - poly * (-x * x).exp())
}

/// Dot product: scalar-scalar, vector-vector.
pub(crate) fn eval_dot(inputs: &[Value]) -> Result<Value, EvalError> {
    let primitive = Primitive::Dot;
    if inputs.len() != 2 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2,
            actual: inputs.len(),
        });
    }

    match (&inputs[0], &inputs[1]) {
        (Value::Scalar(lhs), Value::Scalar(rhs)) => Ok(Value::Scalar(binary_literal_op(
            *lhs,
            *rhs,
            primitive,
            &|a, b| a * b,
            &|a, b| a * b,
        )?)),
        (Value::Tensor(lhs), Value::Tensor(rhs)) => {
            if lhs.rank() != 1 || rhs.rank() != 1 {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: "dot currently supports only rank-1 tensors".to_owned(),
                });
            }
            if lhs.shape != rhs.shape {
                return Err(EvalError::ShapeMismatch {
                    primitive,
                    left: lhs.shape.clone(),
                    right: rhs.shape.clone(),
                });
            }

            if lhs.elements.iter().all(|literal| literal.is_integral())
                && rhs.elements.iter().all(|literal| literal.is_integral())
            {
                let mut sum = 0_i64;
                for (left, right) in lhs.elements.iter().zip(rhs.elements.iter()) {
                    let left_i = left.as_i64().ok_or(EvalError::TypeMismatch {
                        primitive,
                        detail: "integral dot expected i64 elements",
                    })?;
                    let right_i = right.as_i64().ok_or(EvalError::TypeMismatch {
                        primitive,
                        detail: "integral dot expected i64 elements",
                    })?;
                    sum += left_i * right_i;
                }
                return Ok(Value::scalar_i64(sum));
            }

            let mut sum = 0.0_f64;
            for (left, right) in lhs.elements.iter().zip(rhs.elements.iter()) {
                let left_f = left.as_f64().ok_or(EvalError::TypeMismatch {
                    primitive,
                    detail: "expected numeric lhs tensor",
                })?;
                let right_f = right.as_f64().ok_or(EvalError::TypeMismatch {
                    primitive,
                    detail: "expected numeric rhs tensor",
                })?;
                sum += left_f * right_f;
            }
            Ok(Value::scalar_f64(sum))
        }
        _ => Err(EvalError::Unsupported {
            primitive,
            detail: "dot expects either two scalars or two vectors".to_owned(),
        }),
    }
}

/// IsFinite: returns Bool indicating whether each element is finite (not NaN or Inf).
pub(crate) fn eval_is_finite(primitive: Primitive, inputs: &[Value]) -> Result<Value, EvalError> {
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    match &inputs[0] {
        Value::Scalar(literal) => {
            let value = literal.as_f64().ok_or(EvalError::TypeMismatch {
                primitive,
                detail: "expected numeric scalar",
            })?;
            Ok(Value::Scalar(Literal::Bool(value.is_finite())))
        }
        Value::Tensor(tensor) => {
            let elements = tensor
                .elements
                .iter()
                .map(|literal| {
                    literal
                        .as_f64()
                        .map(|v| Literal::Bool(v.is_finite()))
                        .ok_or(EvalError::TypeMismatch {
                            primitive,
                            detail: "expected numeric tensor elements",
                        })
                })
                .collect::<Result<Vec<_>, _>>()?;

            Ok(Value::Tensor(TensorValue::new(
                DType::Bool,
                tensor.shape.clone(),
                elements,
            )?))
        }
    }
}

/// IntegerPow: x.powi(n) where n is an integer exponent from params.
pub(crate) fn eval_integer_pow(
    primitive: Primitive,
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    let exponent: i32 = params
        .get("exponent")
        .and_then(|s| s.trim().parse().ok())
        .ok_or(EvalError::Unsupported {
            primitive,
            detail: "integer_pow requires 'exponent' param".to_owned(),
        })?;

    match &inputs[0] {
        Value::Scalar(literal) => {
            let value = literal.as_f64().ok_or(EvalError::TypeMismatch {
                primitive,
                detail: "expected numeric scalar",
            })?;
            Ok(Value::scalar_f64(value.powi(exponent)))
        }
        Value::Tensor(tensor) => {
            let elements = tensor
                .elements
                .iter()
                .map(|literal| {
                    literal
                        .as_f64()
                        .map(|v| Literal::from_f64(v.powi(exponent)))
                        .ok_or(EvalError::TypeMismatch {
                            primitive,
                            detail: "expected numeric tensor elements",
                        })
                })
                .collect::<Result<Vec<_>, _>>()?;

            Ok(Value::Tensor(TensorValue::new(
                DType::F64,
                tensor.shape.clone(),
                elements,
            )?))
        }
    }
}

/// Nextafter: IEEE 754 next representable float value from x towards y.
pub(crate) fn eval_nextafter(primitive: Primitive, inputs: &[Value]) -> Result<Value, EvalError> {
    if inputs.len() != 2 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2,
            actual: inputs.len(),
        });
    }

    fn next_after(x: f64, y: f64) -> f64 {
        if x.is_nan() || y.is_nan() {
            return f64::NAN;
        }
        if x == y {
            return y;
        }
        if x == 0.0 {
            // Smallest subnormal towards y's sign
            if y > 0.0 {
                return f64::from_bits(1);
            }
            return f64::from_bits(1 | (1_u64 << 63));
        }
        let bits = x.to_bits();
        let result_bits = if (x < y) == (x > 0.0) {
            bits + 1
        } else {
            bits - 1
        };
        f64::from_bits(result_bits)
    }

    match (&inputs[0], &inputs[1]) {
        (Value::Scalar(lhs), Value::Scalar(rhs)) => {
            let x = lhs.as_f64().ok_or(EvalError::TypeMismatch {
                primitive,
                detail: "expected numeric scalar",
            })?;
            let y = rhs.as_f64().ok_or(EvalError::TypeMismatch {
                primitive,
                detail: "expected numeric scalar",
            })?;
            Ok(Value::scalar_f64(next_after(x, y)))
        }
        (Value::Tensor(lhs), Value::Tensor(rhs)) => {
            if lhs.shape != rhs.shape {
                return Err(EvalError::ShapeMismatch {
                    primitive,
                    left: lhs.shape.clone(),
                    right: rhs.shape.clone(),
                });
            }
            let elements = lhs
                .elements
                .iter()
                .zip(rhs.elements.iter())
                .map(|(l, r)| -> Result<Literal, EvalError> {
                    let x = l.as_f64().ok_or(EvalError::TypeMismatch {
                        primitive,
                        detail: "expected numeric tensor elements",
                    })?;
                    let y = r.as_f64().ok_or(EvalError::TypeMismatch {
                        primitive,
                        detail: "expected numeric tensor elements",
                    })?;
                    Ok(Literal::from_f64(next_after(x, y)))
                })
                .collect::<Result<Vec<_>, _>>()?;

            Ok(Value::Tensor(TensorValue::new(
                DType::F64,
                lhs.shape.clone(),
                elements,
            )?))
        }
        _ => Err(EvalError::Unsupported {
            primitive,
            detail: "nextafter requires matching scalar/tensor kinds".to_owned(),
        }),
    }
}
