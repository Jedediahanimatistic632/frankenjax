#![forbid(unsafe_code)]

use fj_core::{DType, Literal, Primitive, Shape, TensorValue, Value};
use std::collections::BTreeMap;

use crate::EvalError;

/// Parse a comma-separated list of i64 values from a param string.
pub(crate) fn parse_i64_param(
    primitive: Primitive,
    key: &str,
    params: &BTreeMap<String, String>,
) -> Result<Vec<i64>, EvalError> {
    let raw = params.get(key).ok_or_else(|| EvalError::Unsupported {
        primitive,
        detail: format!("missing required param '{key}'"),
    })?;
    raw.split(',')
        .map(|s| {
            s.trim().parse::<i64>().map_err(|_| EvalError::Unsupported {
                primitive,
                detail: format!("invalid integer in param '{key}': '{s}'"),
            })
        })
        .collect()
}

/// Parse a comma-separated list of usize values from a param string.
pub(crate) fn parse_usize_param(
    primitive: Primitive,
    key: &str,
    params: &BTreeMap<String, String>,
) -> Result<Vec<usize>, EvalError> {
    let raw = params.get(key).ok_or_else(|| EvalError::Unsupported {
        primitive,
        detail: format!("missing required param '{key}'"),
    })?;
    raw.split(',')
        .map(|s| {
            s.trim()
                .parse::<usize>()
                .map_err(|_| EvalError::Unsupported {
                    primitive,
                    detail: format!("invalid usize in param '{key}': '{s}'"),
                })
        })
        .collect()
}

/// Reshape: change the shape of a tensor without changing its data.
/// Params: `new_shape` (comma-separated dims, -1 for a single inferred axis).
pub(crate) fn eval_reshape(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Reshape;
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    let shape_spec = parse_i64_param(primitive, "new_shape", params)?;

    match &inputs[0] {
        Value::Scalar(lit) => {
            // Scalar -> rank-N tensor (all dims must multiply to 1)
            let mut dims = Vec::with_capacity(shape_spec.len());
            for d in &shape_spec {
                if *d == -1 {
                    dims.push(1_u32);
                } else if *d >= 0 {
                    dims.push(*d as u32);
                } else {
                    return Err(EvalError::Unsupported {
                        primitive,
                        detail: format!("invalid dim {d} in new_shape"),
                    });
                }
            }
            let total: u64 = dims.iter().map(|d| u64::from(*d)).product();
            if total != 1 {
                return Err(EvalError::ShapeMismatch {
                    primitive,
                    left: Shape::scalar(),
                    right: Shape { dims },
                });
            }
            Ok(Value::Tensor(TensorValue::new(
                match lit {
                    Literal::I64(_) => DType::I64,
                    Literal::F64Bits(_) => DType::F64,
                    Literal::Bool(_) => DType::Bool,
                    Literal::Complex64Bits(..) => DType::Complex64,
                    Literal::Complex128Bits(..) => DType::Complex128,
                },
                Shape { dims },
                vec![*lit],
            )?))
        }
        Value::Tensor(tensor) => {
            let elem_count = tensor.elements.len() as u64;
            let mut inferred_axis: Option<usize> = None;
            let mut known_product = 1_u64;
            let mut dims = Vec::with_capacity(shape_spec.len());

            for (idx, d) in shape_spec.iter().enumerate() {
                if *d == -1 {
                    if inferred_axis.is_some() {
                        return Err(EvalError::Unsupported {
                            primitive,
                            detail: "only one -1 inferred axis allowed in new_shape".into(),
                        });
                    }
                    inferred_axis = Some(idx);
                    dims.push(0_u32); // placeholder
                } else if *d >= 0 {
                    let du = *d as u32;
                    known_product *= u64::from(du);
                    dims.push(du);
                } else {
                    return Err(EvalError::Unsupported {
                        primitive,
                        detail: format!("invalid dim {d} in new_shape"),
                    });
                }
            }

            if let Some(axis) = inferred_axis {
                if known_product == 0 || !elem_count.is_multiple_of(known_product) {
                    return Err(EvalError::Unsupported {
                        primitive,
                        detail: format!(
                            "cannot infer dim: elem_count={elem_count} known_product={known_product}"
                        ),
                    });
                }
                dims[axis] = (elem_count / known_product) as u32;
            } else {
                let total: u64 = dims.iter().map(|d| u64::from(*d)).product();
                if total != elem_count {
                    return Err(EvalError::ShapeMismatch {
                        primitive,
                        left: tensor.shape.clone(),
                        right: Shape { dims },
                    });
                }
            }

            Ok(Value::Tensor(TensorValue::new(
                tensor.dtype,
                Shape { dims },
                tensor.elements.clone(),
            )?))
        }
    }
}

/// Transpose: permute the axes of a tensor.
/// Params: `permutation` (comma-separated axis indices). If absent, reverses axes.
pub(crate) fn eval_transpose(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Transpose;
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    match &inputs[0] {
        Value::Scalar(_) => Ok(inputs[0].clone()), // scalar transpose is identity
        Value::Tensor(tensor) => {
            let rank = tensor.shape.rank();
            let permutation = if params.contains_key("permutation") {
                parse_usize_param(primitive, "permutation", params)?
            } else {
                (0..rank).rev().collect()
            };

            if permutation.len() != rank {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!(
                        "permutation length {} does not match rank {}",
                        permutation.len(),
                        rank
                    ),
                });
            }

            // Validate permutation is valid
            let mut seen = vec![false; rank];
            for &p in &permutation {
                if p >= rank || seen[p] {
                    return Err(EvalError::Unsupported {
                        primitive,
                        detail: format!("invalid permutation: {permutation:?} for rank {rank}"),
                    });
                }
                seen[p] = true;
            }

            let old_dims = &tensor.shape.dims;
            let new_dims: Vec<u32> = permutation.iter().map(|&p| old_dims[p]).collect();

            // Compute strides for the original tensor (row-major).
            let mut old_strides = vec![1_usize; rank];
            for i in (0..rank.saturating_sub(1)).rev() {
                old_strides[i] = old_strides[i + 1] * old_dims[i + 1] as usize;
            }

            let total = tensor.elements.len();
            let mut new_elements = vec![Literal::I64(0); total];

            for (flat_idx, elem) in new_elements.iter_mut().enumerate() {
                // Convert flat index to multi-index in new layout.
                let mut remaining = flat_idx;
                let mut old_flat = 0_usize;
                for (new_axis, &perm_axis) in permutation.iter().enumerate() {
                    let new_dim = new_dims[new_axis] as usize;
                    let coord = remaining / {
                        let mut stride = 1;
                        for d in &new_dims[(new_axis + 1)..] {
                            stride *= *d as usize;
                        }
                        stride
                    };
                    remaining %= {
                        let mut stride = 1;
                        for d in &new_dims[(new_axis + 1)..] {
                            stride *= *d as usize;
                        }
                        stride
                    };
                    let _ = new_dim; // used for bounds
                    old_flat += coord * old_strides[perm_axis];
                }
                *elem = tensor.elements[old_flat];
            }

            Ok(Value::Tensor(TensorValue::new(
                tensor.dtype,
                Shape { dims: new_dims },
                new_elements,
            )?))
        }
    }
}

/// BroadcastInDim: broadcast a tensor to a larger shape.
/// Params: `shape` (target dims), `broadcast_dimensions` (mapping of input axes to output axes).
pub(crate) fn eval_broadcast_in_dim(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::BroadcastInDim;
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    let target_dims = parse_i64_param(primitive, "shape", params)?;
    let target_dims: Vec<u32> = target_dims
        .into_iter()
        .map(|d| {
            if d >= 0 {
                Ok(d as u32)
            } else {
                Err(EvalError::Unsupported {
                    primitive,
                    detail: format!("invalid target dim {d}"),
                })
            }
        })
        .collect::<Result<_, _>>()?;

    match &inputs[0] {
        Value::Scalar(lit) => {
            // Broadcast scalar to target shape.
            let total: u64 = target_dims.iter().map(|d| u64::from(*d)).product();
            let elements = vec![*lit; total as usize];
            let dtype = match lit {
                Literal::I64(_) => DType::I64,
                Literal::F64Bits(_) => DType::F64,
                Literal::Bool(_) => DType::Bool,
                Literal::Complex64Bits(..) => DType::Complex64,
                Literal::Complex128Bits(..) => DType::Complex128,
            };
            Ok(Value::Tensor(TensorValue::new(
                dtype,
                Shape { dims: target_dims },
                elements,
            )?))
        }
        Value::Tensor(tensor) => {
            let broadcast_dims = if params.contains_key("broadcast_dimensions") {
                parse_usize_param(primitive, "broadcast_dimensions", params)?
            } else {
                // Default: map input axes to trailing output axes.
                let out_rank = target_dims.len();
                let in_rank = tensor.shape.rank();
                if in_rank > out_rank {
                    return Err(EvalError::Unsupported {
                        primitive,
                        detail: format!("input rank {} exceeds output rank {}", in_rank, out_rank),
                    });
                }
                (out_rank - in_rank..out_rank).collect()
            };

            if broadcast_dims.len() != tensor.shape.rank() {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!(
                        "broadcast_dimensions length {} must equal input rank {}",
                        broadcast_dims.len(),
                        tensor.shape.rank()
                    ),
                });
            }

            let out_rank = target_dims.len();
            let total: usize = target_dims.iter().map(|d| *d as usize).product();

            // Build mapping: for each output axis, which input axis maps to it (if any).
            let mut out_to_in: Vec<Option<usize>> = vec![None; out_rank];
            for (in_axis, &out_axis) in broadcast_dims.iter().enumerate() {
                out_to_in[out_axis] = Some(in_axis);
            }

            // Compute input strides (row-major).
            let in_dims = &tensor.shape.dims;
            let mut in_strides = vec![1_usize; tensor.shape.rank()];
            for i in (0..tensor.shape.rank().saturating_sub(1)).rev() {
                in_strides[i] = in_strides[i + 1] * in_dims[i + 1] as usize;
            }

            let mut elements = Vec::with_capacity(total);
            let mut out_coords = vec![0_usize; out_rank];

            for _ in 0..total {
                // Map output coords to input index.
                let mut in_flat = 0_usize;
                for (out_axis, mapping) in out_to_in.iter().enumerate() {
                    if let Some(in_axis) = mapping {
                        let in_dim = in_dims[*in_axis] as usize;
                        // If input dim is 1, broadcast (coord maps to 0).
                        let coord = if in_dim == 1 { 0 } else { out_coords[out_axis] };
                        in_flat += coord * in_strides[*in_axis];
                    }
                }
                elements.push(tensor.elements[in_flat]);

                // Increment output coordinates (row-major order).
                for axis in (0..out_rank).rev() {
                    out_coords[axis] += 1;
                    if out_coords[axis] < target_dims[axis] as usize {
                        break;
                    }
                    out_coords[axis] = 0;
                }
            }

            Ok(Value::Tensor(TensorValue::new(
                tensor.dtype,
                Shape { dims: target_dims },
                elements,
            )?))
        }
    }
}

/// Concatenate: join multiple tensors along an axis.
/// Params: `dimension` (axis index, default 0).
pub(crate) fn eval_concatenate(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Concatenate;
    if inputs.is_empty() {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: 0,
        });
    }

    let axis: usize = params
        .get("dimension")
        .map(|s| {
            s.parse::<usize>().map_err(|_| EvalError::Unsupported {
                primitive,
                detail: format!("invalid dimension param: '{s}'"),
            })
        })
        .transpose()?
        .unwrap_or(0);

    // Collect all inputs as tensors. Scalars are invalid for concatenation.
    let tensors: Vec<&TensorValue> = inputs
        .iter()
        .map(|v| match v {
            Value::Tensor(t) => Ok(t),
            Value::Scalar(_) => Err(EvalError::Unsupported {
                primitive,
                detail: "cannot concatenate scalars".into(),
            }),
        })
        .collect::<Result<_, _>>()?;

    let rank = tensors[0].shape.rank();
    if axis >= rank {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!("concat axis {axis} out of bounds for rank {rank}"),
        });
    }

    // Validate: all tensors must have the same rank and matching dims on non-concat axes.
    for (i, t) in tensors.iter().enumerate().skip(1) {
        if t.shape.rank() != rank {
            return Err(EvalError::ShapeMismatch {
                primitive,
                left: tensors[0].shape.clone(),
                right: t.shape.clone(),
            });
        }
        for (ax, (d0, di)) in tensors[0]
            .shape
            .dims
            .iter()
            .zip(t.shape.dims.iter())
            .enumerate()
        {
            if ax != axis && d0 != di {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!(
                        "tensor {i} dim mismatch on axis {ax}: expected {d0}, got {di}"
                    ),
                });
            }
        }
    }

    // Build output shape.
    let mut out_dims = tensors[0].shape.dims.clone();
    let concat_size: u32 = tensors.iter().map(|t| t.shape.dims[axis]).sum();
    out_dims[axis] = concat_size;

    // For concatenation, iterate over all output coordinates.
    let total: usize = out_dims.iter().map(|d| *d as usize).product();
    let mut elements = Vec::with_capacity(total);
    let mut out_coords = vec![0_usize; rank];

    for _ in 0..total {
        // Determine which input tensor and its local coord on the concat axis.
        let mut concat_coord = out_coords[axis];
        let mut tensor_idx = 0;
        for (i, t) in tensors.iter().enumerate() {
            let dim = t.shape.dims[axis] as usize;
            if concat_coord < dim {
                tensor_idx = i;
                break;
            }
            concat_coord -= dim;
        }

        // Compute flat index into the selected tensor.
        let t = tensors[tensor_idx];
        let mut flat = 0_usize;
        let mut stride = 1_usize;
        for ax in (0..rank).rev() {
            let coord = if ax == axis {
                concat_coord
            } else {
                out_coords[ax]
            };
            flat += coord * stride;
            stride *= t.shape.dims[ax] as usize;
        }
        elements.push(t.elements[flat]);

        // Increment output coordinates.
        for ax in (0..rank).rev() {
            out_coords[ax] += 1;
            if out_coords[ax] < out_dims[ax] as usize {
                break;
            }
            out_coords[ax] = 0;
        }
    }

    let dtype = tensors[0].dtype;
    Ok(Value::Tensor(TensorValue::new(
        dtype,
        Shape { dims: out_dims },
        elements,
    )?))
}

/// Pad: add low/high edge padding and interior padding between elements.
///
/// Inputs: `[operand, pad_value]`
/// Params:
/// - `padding_low`: comma-separated non-negative integers (one per axis)
/// - `padding_high`: comma-separated non-negative integers (one per axis)
/// - `padding_interior`: comma-separated non-negative integers (one per axis, optional; defaults to 0)
pub(crate) fn eval_pad(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Pad;
    if inputs.len() != 2 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2,
            actual: inputs.len(),
        });
    }

    let operand = match &inputs[0] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "operand must be a tensor".into(),
            });
        }
    };

    let pad_literal = match &inputs[1] {
        Value::Scalar(lit) => *lit,
        Value::Tensor(tensor) if tensor.elements.len() == 1 => tensor.elements[0],
        _ => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "pad value must be a scalar literal".into(),
            });
        }
    };

    let rank = operand.shape.rank();
    let lows_raw = parse_i64_param(primitive, "padding_low", params)?;
    let highs_raw = parse_i64_param(primitive, "padding_high", params)?;
    let interiors_raw = if params.contains_key("padding_interior") {
        parse_i64_param(primitive, "padding_interior", params)?
    } else {
        vec![0_i64; rank]
    };

    if lows_raw.len() != rank || highs_raw.len() != rank || interiors_raw.len() != rank {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!(
                "padding params must match rank {rank}: low={} high={} interior={}",
                lows_raw.len(),
                highs_raw.len(),
                interiors_raw.len()
            ),
        });
    }

    let mut lows = Vec::with_capacity(rank);
    let mut interiors = Vec::with_capacity(rank);
    let mut out_dims = Vec::with_capacity(rank);

    for ax in 0..rank {
        let low = lows_raw[ax];
        let high = highs_raw[ax];
        let interior = interiors_raw[ax];
        if low < 0 || high < 0 || interior < 0 {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!(
                    "padding values must be non-negative on axis {ax}: low={low} high={high} interior={interior}"
                ),
            });
        }

        let dim = i64::from(operand.shape.dims[ax]);
        let interior_span = if dim == 0 { 0 } else { (dim - 1) * interior };
        let out_dim = low + dim + interior_span + high;
        if out_dim < 0 || out_dim > i64::from(u32::MAX) {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!("padded dimension overflow on axis {ax}: {out_dim}"),
            });
        }

        lows.push(low as usize);
        interiors.push(interior as usize);
        out_dims.push(out_dim as u32);
    }

    let out_total: usize = out_dims.iter().map(|d| *d as usize).product();
    let mut out_elements = vec![pad_literal; out_total];

    if rank == 0 {
        if !out_elements.is_empty() {
            out_elements[0] = operand.elements[0];
        }
        return Ok(Value::Tensor(TensorValue::new(
            operand.dtype,
            Shape { dims: out_dims },
            out_elements,
        )?));
    }

    // Row-major strides.
    let in_dims = &operand.shape.dims;
    let mut out_strides = vec![1_usize; rank];
    for ax in (0..rank.saturating_sub(1)).rev() {
        out_strides[ax] = out_strides[ax + 1] * out_dims[ax + 1] as usize;
    }

    let mut in_coords = vec![0_usize; rank];
    for element in &operand.elements {
        let mut out_flat = 0_usize;
        for ax in 0..rank {
            let coord = lows[ax] + in_coords[ax] * (interiors[ax] + 1);
            out_flat += coord * out_strides[ax];
        }
        out_elements[out_flat] = *element;

        for ax in (0..rank).rev() {
            in_coords[ax] += 1;
            if in_coords[ax] < in_dims[ax] as usize {
                break;
            }
            in_coords[ax] = 0;
        }
    }

    Ok(Value::Tensor(TensorValue::new(
        operand.dtype,
        Shape { dims: out_dims },
        out_elements,
    )?))
}

/// Slice: extract a sub-tensor.
/// Params: `start_indices` and `limit_indices` (comma-separated u32 lists).
pub(crate) fn eval_slice(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Slice;
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    match &inputs[0] {
        Value::Scalar(_) => Err(EvalError::Unsupported {
            primitive,
            detail: "cannot slice a scalar".into(),
        }),
        Value::Tensor(tensor) => {
            let starts = parse_usize_param(primitive, "start_indices", params)?;
            let limits = parse_usize_param(primitive, "limit_indices", params)?;

            let rank = tensor.shape.rank();
            if starts.len() != rank || limits.len() != rank {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!(
                        "start/limit rank mismatch: starts={} limits={} rank={}",
                        starts.len(),
                        limits.len(),
                        rank
                    ),
                });
            }

            // Validate and compute output dims.
            let mut out_dims = Vec::with_capacity(rank);
            for ax in 0..rank {
                let dim = tensor.shape.dims[ax] as usize;
                if starts[ax] > limits[ax] || limits[ax] > dim {
                    return Err(EvalError::Unsupported {
                        primitive,
                        detail: format!(
                            "invalid slice on axis {ax}: start={} limit={} dim={}",
                            starts[ax], limits[ax], dim
                        ),
                    });
                }
                out_dims.push((limits[ax] - starts[ax]) as u32);
            }

            // Compute input strides (row-major).
            let in_dims = &tensor.shape.dims;
            let mut in_strides = vec![1_usize; rank];
            for i in (0..rank.saturating_sub(1)).rev() {
                in_strides[i] = in_strides[i + 1] * in_dims[i + 1] as usize;
            }

            let total: usize = out_dims.iter().map(|d| *d as usize).product();
            let mut elements = Vec::with_capacity(total);
            let mut out_coords = vec![0_usize; rank];

            for _ in 0..total {
                // Map output coords to input coords by adding start offsets.
                let mut in_flat = 0_usize;
                for ax in 0..rank {
                    in_flat += (out_coords[ax] + starts[ax]) * in_strides[ax];
                }
                elements.push(tensor.elements[in_flat]);

                // Increment output coordinates.
                for ax in (0..rank).rev() {
                    out_coords[ax] += 1;
                    if out_coords[ax] < out_dims[ax] as usize {
                        break;
                    }
                    out_coords[ax] = 0;
                }
            }

            Ok(Value::Tensor(TensorValue::new(
                tensor.dtype,
                Shape { dims: out_dims },
                elements,
            )?))
        }
    }
}

/// Gather: index into an operand tensor using an indices tensor.
///
/// Simplified semantics (1-D index gather):
///   operand: tensor of any rank
///   indices: 1-D integer tensor of gather indices (into axis 0 of operand)
///   params:  `slice_sizes` — comma-separated sizes for each axis of the gathered slice
///
/// For each index i in `indices`, extracts a slice of shape `slice_sizes` starting
/// at position `[indices[i], 0, 0, ...]` from `operand`.
/// Output shape: `[num_indices] ++ slice_sizes[1..]` (the leading axis is replaced by
/// the number of gathered indices, remaining axes keep their slice sizes).
pub(crate) fn eval_gather(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Gather;
    if inputs.len() != 2 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2,
            actual: inputs.len(),
        });
    }

    let operand = match &inputs[0] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "cannot gather from a scalar".into(),
            });
        }
    };

    if operand.shape.rank() == 0 {
        return Err(EvalError::Unsupported {
            primitive,
            detail: "cannot gather from a rank-0 tensor".into(),
        });
    }

    let slice_sizes = parse_usize_param(primitive, "slice_sizes", params)?;
    if slice_sizes.len() != operand.shape.rank() {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!(
                "slice_sizes length {} must equal operand rank {}",
                slice_sizes.len(),
                operand.shape.rank()
            ),
        });
    }

    // Extract indices as flat i64 values
    let index_vals: Vec<usize> = match &inputs[1] {
        Value::Scalar(lit) => {
            vec![lit_to_usize(lit, primitive)?]
        }
        Value::Tensor(t) => t
            .elements
            .iter()
            .map(|lit| lit_to_usize(lit, primitive))
            .collect::<Result<_, _>>()?,
    };

    let num_indices = index_vals.len();
    let rank = operand.shape.rank();

    // Compute operand strides (row-major)
    let op_dims = &operand.shape.dims;

    for (ax, (&ss, &dim)) in slice_sizes.iter().zip(op_dims.iter()).enumerate() {
        if ss > dim as usize {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!("slice_sizes[{ax}] = {ss} exceeds operand dimension {dim}"),
            });
        }
    }

    let mut op_strides = vec![1_usize; rank];
    for i in (0..rank.saturating_sub(1)).rev() {
        op_strides[i] = op_strides[i + 1] * op_dims[i + 1] as usize;
    }

    // Output shape: [num_indices, slice_sizes[1], slice_sizes[2], ...]
    let mut out_dims: Vec<u32> = vec![num_indices as u32];
    for &s in &slice_sizes[1..] {
        out_dims.push(s as u32);
    }

    // Number of elements per gathered slice (product of slice_sizes[1..])
    let slice_elems: usize = slice_sizes[1..].iter().product::<usize>();

    let total = num_indices * slice_elems;
    let mut elements = Vec::with_capacity(total);

    for &idx in &index_vals {
        if idx >= op_dims[0] as usize {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!(
                    "gather index {} out of bounds for axis 0 dim {}",
                    idx, op_dims[0]
                ),
            });
        }

        // Base offset for this index along axis 0
        let base_offset = idx * op_strides[0];

        // Iterate over all positions within the slice (axes 1..rank)
        let mut slice_coords = vec![0_usize; rank.saturating_sub(1)];
        for _ in 0..slice_elems {
            let mut flat = base_offset;
            for (ax, &coord) in slice_coords.iter().enumerate() {
                flat += coord * op_strides[ax + 1];
            }
            elements.push(operand.elements[flat]);

            // Increment slice coordinates
            if rank > 1 {
                for ax in (0..rank - 1).rev() {
                    slice_coords[ax] += 1;
                    if slice_coords[ax] < slice_sizes[ax + 1] {
                        break;
                    }
                    slice_coords[ax] = 0;
                }
            }
        }
    }

    Ok(Value::Tensor(TensorValue::new(
        operand.dtype,
        Shape { dims: out_dims },
        elements,
    )?))
}

/// Scatter: update positions in an operand tensor using indices and update values.
///
/// Simplified semantics (1-D index scatter):
///   operand: tensor to scatter into (cloned, not mutated)
///   indices: 1-D integer tensor of scatter indices (into axis 0 of operand)
///   updates: tensor of values to scatter; shape = [num_indices] ++ operand.shape[1..]
///
/// For each index i in `indices`, overwrites the slice at `operand[indices[i], ...]`
/// with the corresponding slice from `updates[i, ...]`.
pub(crate) fn eval_scatter(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Scatter;
    let _ = params; // No params needed for basic scatter
    if inputs.len() != 3 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 3,
            actual: inputs.len(),
        });
    }

    let operand = match &inputs[0] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "cannot scatter into a scalar".into(),
            });
        }
    };

    if operand.shape.rank() == 0 {
        return Err(EvalError::Unsupported {
            primitive,
            detail: "cannot scatter into a rank-0 tensor".into(),
        });
    }

    let index_vals: Vec<usize> = match &inputs[1] {
        Value::Scalar(lit) => vec![lit_to_usize(lit, primitive)?],
        Value::Tensor(t) => t
            .elements
            .iter()
            .map(|lit| lit_to_usize(lit, primitive))
            .collect::<Result<_, _>>()?,
    };

    let updates = match &inputs[2] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "updates must be a tensor".into(),
            });
        }
    };

    let rank = operand.shape.rank();
    let op_dims = &operand.shape.dims;

    // Compute operand strides (row-major)
    let mut op_strides = vec![1_usize; rank];
    for i in (0..rank.saturating_sub(1)).rev() {
        op_strides[i] = op_strides[i + 1] * op_dims[i + 1] as usize;
    }

    // Number of elements per slice (product of dims[1..])
    let slice_elems: usize = op_dims[1..].iter().map(|d| *d as usize).product::<usize>();

    // Clone operand elements to create output
    let mut result_elements = operand.elements.clone();

    let mode = params
        .get("mode")
        .map(|s| s.as_str())
        .unwrap_or("overwrite");

    if mode != "overwrite" && mode != "add" {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!("unknown scatter mode \"{mode}\", expected \"overwrite\" or \"add\""),
        });
    }

    let expected_update_elems = index_vals.len() * slice_elems;
    if updates.elements.len() != expected_update_elems {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!(
                "updates has {} elements but expected {} (num_indices={} * slice_elems={slice_elems})",
                updates.elements.len(),
                expected_update_elems,
                index_vals.len()
            ),
        });
    }

    for (i, &idx) in index_vals.iter().enumerate() {
        if idx >= op_dims[0] as usize {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!(
                    "scatter index {} out of bounds for axis 0 dim {}",
                    idx, op_dims[0]
                ),
            });
        }

        let base_offset = idx * op_strides[0];
        let update_offset = i * slice_elems;

        for j in 0..slice_elems {
            let current = &result_elements[base_offset + j];
            let update = &updates.elements[update_offset + j];
            if mode == "add" {
                let c_val = current.as_f64().unwrap_or(0.0);
                let u_val = update.as_f64().unwrap_or(0.0);
                result_elements[base_offset + j] = Literal::from_f64(c_val + u_val);
            } else {
                result_elements[base_offset + j] = *update;
            }
        }
    }

    Ok(Value::Tensor(TensorValue::new(
        operand.dtype,
        operand.shape.clone(),
        result_elements,
    )?))
}

fn lit_to_usize(lit: &Literal, primitive: Primitive) -> Result<usize, EvalError> {
    match lit {
        Literal::I64(n) => {
            if *n >= 0 {
                Ok(*n as usize)
            } else {
                Err(EvalError::Unsupported {
                    primitive,
                    detail: format!("negative index {n}"),
                })
            }
        }
        Literal::Bool(b) => Ok(if *b { 1 } else { 0 }),
        Literal::F64Bits(_) => Err(EvalError::Unsupported {
            primitive,
            detail: "float indices not supported".into(),
        }),
        Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => Err(EvalError::Unsupported {
            primitive,
            detail: "complex indices not supported".into(),
        }),
    }
}

/// Dynamic slice: like slice but start indices are dynamic (runtime) values.
///
/// Inputs: [operand, start_0, start_1, ...] where start_i are scalar indices.
/// Params: `slice_sizes` — comma-separated sizes for the output slice along each axis.
///
/// JAX semantics: start indices are clamped to valid range [0, dim - size].
pub(crate) fn eval_dynamic_slice(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::DynamicSlice;
    if inputs.is_empty() {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2,
            actual: 0,
        });
    }

    let tensor = match &inputs[0] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "cannot dynamic_slice a scalar".into(),
            });
        }
    };

    let rank = tensor.shape.rank();
    let slice_sizes = parse_usize_param(primitive, "slice_sizes", params)?;

    if slice_sizes.len() != rank {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!(
                "slice_sizes length {} does not match operand rank {}",
                slice_sizes.len(),
                rank
            ),
        });
    }

    // Start indices come from remaining inputs (one scalar per axis)
    if inputs.len() != 1 + rank {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1 + rank,
            actual: inputs.len(),
        });
    }

    let mut starts = Vec::with_capacity(rank);
    for ax in 0..rank {
        let start_val = match &inputs[1 + ax] {
            Value::Scalar(lit) => {
                let raw = match lit {
                    Literal::I64(v) => *v,
                    Literal::F64Bits(b) => f64::from_bits(*b) as i64,
                    Literal::Bool(b) => {
                        if *b {
                            1
                        } else {
                            0
                        }
                    }
                    Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                        return Err(EvalError::Unsupported {
                            primitive,
                            detail: format!("complex start index not supported for axis {ax}"),
                        });
                    }
                };
                // Clamp to valid range [0, dim - size]
                let dim = tensor.shape.dims[ax] as i64;
                let size = slice_sizes[ax] as i64;
                let clamped = raw.max(0).min(dim - size);
                clamped as usize
            }
            Value::Tensor(_) => {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!("start index for axis {ax} must be a scalar"),
                });
            }
        };
        starts.push(start_val);
    }

    // Validate slice sizes
    for ax in 0..rank {
        if starts[ax] + slice_sizes[ax] > tensor.shape.dims[ax] as usize {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!(
                    "dynamic_slice out of bounds on axis {ax}: start={} size={} dim={}",
                    starts[ax], slice_sizes[ax], tensor.shape.dims[ax]
                ),
            });
        }
    }

    // Compute strides
    let mut in_strides = vec![1_usize; rank];
    for i in (0..rank.saturating_sub(1)).rev() {
        in_strides[i] = in_strides[i + 1] * tensor.shape.dims[i + 1] as usize;
    }

    let total: usize = slice_sizes.iter().product();
    let mut elements = Vec::with_capacity(total);
    let mut out_coords = vec![0_usize; rank];

    for _ in 0..total {
        let mut in_flat = 0_usize;
        for ax in 0..rank {
            in_flat += (out_coords[ax] + starts[ax]) * in_strides[ax];
        }
        elements.push(tensor.elements[in_flat]);

        // Increment output coordinates
        for ax in (0..rank).rev() {
            out_coords[ax] += 1;
            if out_coords[ax] < slice_sizes[ax] {
                break;
            }
            out_coords[ax] = 0;
        }
    }

    let out_dims: Vec<u32> = slice_sizes.iter().map(|&s| s as u32).collect();
    Ok(Value::Tensor(TensorValue::new(
        tensor.dtype,
        Shape { dims: out_dims },
        elements,
    )?))
}

/// DynamicUpdateSlice: write `update` into `operand` at position given by start indices.
///
/// Inputs: [operand, update, start_0, start_1, ..., start_{rank-1}]
/// Output: tensor with same shape as operand, with the update region overwritten.
pub(crate) fn eval_dynamic_update_slice(
    inputs: &[Value],
    _params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::DynamicUpdateSlice;
    if inputs.len() < 2 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 3,
            actual: inputs.len(),
        });
    }

    let operand = match &inputs[0] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "cannot dynamic_update_slice a scalar".into(),
            });
        }
    };

    let update = match &inputs[1] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "update must be a tensor".into(),
            });
        }
    };

    let rank = operand.shape.rank();
    if update.shape.rank() != rank {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!(
                "update rank {} != operand rank {}",
                update.shape.rank(),
                rank
            ),
        });
    }

    if inputs.len() != 2 + rank {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2 + rank,
            actual: inputs.len(),
        });
    }

    // Parse start indices (one scalar per axis), clamped to valid range
    let mut starts = Vec::with_capacity(rank);
    for ax in 0..rank {
        let start_val = match &inputs[2 + ax] {
            Value::Scalar(lit) => {
                let raw = match lit {
                    Literal::I64(v) => *v,
                    Literal::F64Bits(b) => f64::from_bits(*b) as i64,
                    Literal::Bool(b) => i64::from(*b),
                    Literal::Complex64Bits(..) | Literal::Complex128Bits(..) => {
                        return Err(EvalError::Unsupported {
                            primitive,
                            detail: format!("complex start index not supported for axis {ax}"),
                        });
                    }
                };
                let dim = operand.shape.dims[ax] as i64;
                let upd_size = update.shape.dims[ax] as i64;
                raw.max(0).min(dim - upd_size) as usize
            }
            Value::Tensor(_) => {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!("start index for axis {ax} must be a scalar"),
                });
            }
        };
        starts.push(start_val);
    }

    // Copy operand elements, overwriting the update region
    let mut elements = operand.elements.clone();

    let mut op_strides = vec![1_usize; rank];
    for i in (0..rank.saturating_sub(1)).rev() {
        op_strides[i] = op_strides[i + 1] * operand.shape.dims[i + 1] as usize;
    }

    let upd_total = update.elements.len();
    let mut upd_coords = vec![0_usize; rank];

    for upd_flat in 0..upd_total {
        let mut op_flat = 0_usize;
        for ax in 0..rank {
            op_flat += (upd_coords[ax] + starts[ax]) * op_strides[ax];
        }
        if op_flat < elements.len() {
            elements[op_flat] = update.elements[upd_flat];
        }

        // Increment update coordinates
        for ax in (0..rank).rev() {
            upd_coords[ax] += 1;
            if upd_coords[ax] < update.shape.dims[ax] as usize {
                break;
            }
            upd_coords[ax] = 0;
        }
    }

    Ok(Value::Tensor(TensorValue::new(
        operand.dtype,
        operand.shape.clone(),
        elements,
    )?))
}

/// Iota: generate a 1-D index tensor of length `length` with dtype `dtype`.
///
/// Inputs: [] (no inputs — iota is a nullary operation)
/// Params: `length` — number of elements, `dtype` — element type (default: I64)
pub(crate) fn eval_iota(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::Iota;
    if !inputs.is_empty() {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 0,
            actual: inputs.len(),
        });
    }

    let length_str = params.get("length").ok_or_else(|| EvalError::Unsupported {
        primitive,
        detail: "missing required param 'length'".into(),
    })?;
    let length: u32 = length_str
        .trim()
        .parse()
        .map_err(|_| EvalError::Unsupported {
            primitive,
            detail: format!("invalid length: '{length_str}'"),
        })?;

    let dtype_str = params.get("dtype").map(String::as_str).unwrap_or("I64");
    let (dtype, elements) = match dtype_str {
        "I64" | "i64" => {
            let elems: Vec<Literal> = (0..length as i64).map(Literal::I64).collect();
            (DType::I64, elems)
        }
        "I32" | "i32" => {
            let elems: Vec<Literal> = (0..length as i64).map(Literal::I64).collect();
            (DType::I32, elems)
        }
        "F64" | "f64" => {
            let elems: Vec<Literal> = (0..length)
                .map(|i| Literal::from_f64(f64::from(i)))
                .collect();
            (DType::F64, elems)
        }
        "F32" | "f32" => {
            let elems: Vec<Literal> = (0..length)
                .map(|i| Literal::from_f64(f64::from(i)))
                .collect();
            (DType::F32, elems)
        }
        _ => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!("unsupported dtype for iota: '{dtype_str}'"),
            });
        }
    };

    Ok(Value::Tensor(TensorValue::new(
        dtype,
        Shape::vector(length),
        elements,
    )?))
}

/// One-hot encoding: given integer indices, produces a tensor with a trailing
/// `num_classes` dimension where position `index` is `on_value` and the rest
/// are `off_value`.
pub(crate) fn eval_one_hot(
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    let primitive = Primitive::OneHot;
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    let num_classes_str = params
        .get("num_classes")
        .ok_or_else(|| EvalError::Unsupported {
            primitive,
            detail: "missing required param 'num_classes'".into(),
        })?;
    let num_classes: u32 = num_classes_str
        .trim()
        .parse()
        .map_err(|_| EvalError::Unsupported {
            primitive,
            detail: format!("invalid num_classes: '{num_classes_str}'"),
        })?;

    let on_value: f64 = params
        .get("on_value")
        .and_then(|s| s.trim().parse().ok())
        .unwrap_or(1.0);
    let off_value: f64 = params
        .get("off_value")
        .and_then(|s| s.trim().parse().ok())
        .unwrap_or(0.0);

    let dtype_str = params.get("dtype").map(String::as_str).unwrap_or("F64");

    // Collect flat index values from input
    let indices: Vec<i64> = match &inputs[0] {
        Value::Scalar(lit) => {
            vec![lit.as_i64().ok_or(EvalError::TypeMismatch {
                primitive,
                detail: "one_hot requires integer indices",
            })?]
        }
        Value::Tensor(t) => {
            let mut idxs = Vec::with_capacity(t.elements.len());
            for lit in &t.elements {
                idxs.push(lit.as_i64().ok_or(EvalError::TypeMismatch {
                    primitive,
                    detail: "one_hot requires integer indices",
                })?);
            }
            idxs
        }
    };

    // Build output shape: input_shape ++ [num_classes]
    let input_shape = match &inputs[0] {
        Value::Scalar(_) => vec![],
        Value::Tensor(t) => t.shape.dims.clone(),
    };
    let mut out_dims = input_shape;
    out_dims.push(num_classes);

    let nc = num_classes as usize;
    let total = indices.len() * nc;
    let mut elements = Vec::with_capacity(total);

    let use_int = matches!(dtype_str, "I64" | "i64" | "I32" | "i32");

    for &idx in &indices {
        for c in 0..nc {
            let val = if idx >= 0 && (idx as usize) == c {
                on_value
            } else {
                off_value
            };
            if use_int {
                elements.push(Literal::I64(val as i64));
            } else {
                elements.push(Literal::from_f64(val));
            }
        }
    }

    let dtype = match dtype_str {
        "I64" | "i64" => DType::I64,
        "I32" | "i32" => DType::I32,
        "F32" | "f32" => DType::F32,
        _ => DType::F64,
    };

    Ok(Value::Tensor(TensorValue::new(
        dtype,
        Shape { dims: out_dims },
        elements,
    )?))
}

/// Sort: sort elements along a specified axis (default: last axis).
pub(crate) fn eval_sort(
    primitive: Primitive,
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    let descending = params
        .get("descending")
        .map(|s| s.trim() == "true")
        .unwrap_or(false);

    match &inputs[0] {
        Value::Scalar(_) => Ok(inputs[0].clone()),
        Value::Tensor(tensor) => {
            let rank = tensor.shape.rank();
            if rank == 0 {
                return Ok(Value::Scalar(tensor.elements[0]));
            }

            let axis: usize = params
                .get("axis")
                .and_then(|s| s.trim().parse().ok())
                .unwrap_or(rank - 1);

            if axis >= rank {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!("axis {} out of bounds for rank {}", axis, rank),
                });
            }

            sort_along_axis(tensor, axis, descending, false).map_err(|e| EvalError::Unsupported {
                primitive,
                detail: e,
            })
        }
    }
}

/// Argsort: return indices that would sort along a specified axis.
pub(crate) fn eval_argsort(
    primitive: Primitive,
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    if inputs.len() != 1 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 1,
            actual: inputs.len(),
        });
    }

    let descending = params
        .get("descending")
        .map(|s| s.trim() == "true")
        .unwrap_or(false);

    match &inputs[0] {
        Value::Scalar(_) => Ok(Value::scalar_i64(0)),
        Value::Tensor(tensor) => {
            let rank = tensor.shape.rank();
            if rank == 0 {
                return Ok(Value::scalar_i64(0));
            }

            let axis: usize = params
                .get("axis")
                .and_then(|s| s.trim().parse().ok())
                .unwrap_or(rank - 1);

            if axis >= rank {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!("axis {} out of bounds for rank {}", axis, rank),
                });
            }

            sort_along_axis(tensor, axis, descending, true).map_err(|e| EvalError::Unsupported {
                primitive,
                detail: e,
            })
        }
    }
}

/// Sort or argsort a tensor along a given axis.
fn sort_along_axis(
    tensor: &TensorValue,
    axis: usize,
    descending: bool,
    return_indices: bool,
) -> Result<Value, String> {
    let rank = tensor.shape.rank();
    let axis_dim = tensor.shape.dims[axis] as usize;

    let mut strides = vec![1_usize; rank];
    for i in (0..rank.saturating_sub(1)).rev() {
        strides[i] = strides[i + 1] * tensor.shape.dims[i + 1] as usize;
    }
    let axis_stride = strides[axis];
    let total = tensor.elements.len();
    let outer_count = total / axis_dim;

    let is_integral = tensor.dtype == DType::I64 || tensor.dtype == DType::I32;
    let mut result_elements = if return_indices {
        vec![Literal::I64(0); total]
    } else {
        tensor.elements.clone()
    };

    for outer in 0..outer_count {
        let base = {
            let mut idx = outer;
            let mut flat = 0_usize;
            for ax in (0..rank).rev() {
                if ax == axis {
                    continue;
                }
                let dim = tensor.shape.dims[ax] as usize;
                flat += (idx % dim) * strides[ax];
                idx /= dim;
            }
            flat
        };

        let mut indexed: Vec<(usize, f64)> = (0..axis_dim)
            .map(|i| {
                let flat_idx = base + i * axis_stride;
                let val = if is_integral {
                    tensor.elements[flat_idx]
                        .as_i64()
                        .map(|v| v as f64)
                        .unwrap_or(0.0)
                } else {
                    tensor.elements[flat_idx].as_f64().unwrap_or(0.0)
                };
                (i, val)
            })
            .collect();

        if descending {
            indexed.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
        } else {
            indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));
        }

        for (out_pos, &(orig_idx, val)) in indexed.iter().enumerate() {
            let flat_idx = base + out_pos * axis_stride;
            if return_indices {
                result_elements[flat_idx] = Literal::I64(orig_idx as i64);
            } else if is_integral {
                result_elements[flat_idx] = Literal::I64(val as i64);
            } else {
                result_elements[flat_idx] = Literal::from_f64(val);
            }
        }
    }

    let out_dtype = if return_indices {
        DType::I64
    } else {
        tensor.dtype
    };
    Ok(Value::Tensor(
        TensorValue::new(out_dtype, tensor.shape.clone(), result_elements)
            .map_err(|e| e.to_string())?,
    ))
}

/// Conv: N-dimensional convolution.
/// Layout: lhs=[batch, spatial..., in_channels], rhs=[kernel_spatial..., in_channels, out_channels]
/// Params: strides (comma-sep), padding ("valid" or "same")
pub(crate) fn eval_conv(
    primitive: Primitive,
    inputs: &[Value],
    params: &BTreeMap<String, String>,
) -> Result<Value, EvalError> {
    if inputs.len() != 2 {
        return Err(EvalError::ArityMismatch {
            primitive,
            expected: 2,
            actual: inputs.len(),
        });
    }

    let lhs = match &inputs[0] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "conv requires tensor inputs".into(),
            });
        }
    };
    let rhs = match &inputs[1] {
        Value::Tensor(t) => t,
        Value::Scalar(_) => {
            return Err(EvalError::Unsupported {
                primitive,
                detail: "conv requires tensor kernel".into(),
            });
        }
    };

    let lhs_rank = lhs.shape.rank();
    if lhs_rank < 3 {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!("conv lhs must have rank >= 3, got {lhs_rank}"),
        });
    }

    if lhs_rank > 4 {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!("conv supports rank 3 (1D) and rank 4 (2D), got rank {lhs_rank}"),
        });
    }

    let padding_mode = params.get("padding").map(String::as_str).unwrap_or("valid");

    if lhs_rank == 3 {
        eval_conv_1d(primitive, lhs, rhs, params, padding_mode)
    } else {
        eval_conv_2d(primitive, lhs, rhs, params, padding_mode)
    }
}

/// 1D convolution: lhs=[N, W, C_in], rhs=[K, C_in, C_out]
fn eval_conv_1d(
    primitive: Primitive,
    lhs: &TensorValue,
    rhs: &TensorValue,
    params: &BTreeMap<String, String>,
    padding_mode: &str,
) -> Result<Value, EvalError> {
    if rhs.shape.rank() != 3 {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!(
                "1D conv kernel must have rank 3 [K, C_in, C_out], got rank {}",
                rhs.shape.rank()
            ),
        });
    }

    let batch = lhs.shape.dims[0] as usize;
    let width = lhs.shape.dims[1] as usize;
    let c_in = lhs.shape.dims[2] as usize;

    let kernel_w = rhs.shape.dims[0] as usize;
    let rhs_c_in = rhs.shape.dims[1] as usize;
    let c_out = rhs.shape.dims[2] as usize;

    if c_in != rhs_c_in {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!("channel mismatch: lhs c_in={c_in}, rhs c_in={rhs_c_in}"),
        });
    }

    let stride: usize = params
        .get("strides")
        .and_then(|s| s.trim().parse().ok())
        .unwrap_or(1);

    let (out_w, pad_left) = match padding_mode {
        "same" | "SAME" => {
            let out_w = width.div_ceil(stride);
            let pad_total = ((out_w - 1) * stride + kernel_w).saturating_sub(width);
            (out_w, pad_total / 2)
        }
        _ => {
            if width < kernel_w {
                return Err(EvalError::Unsupported {
                    primitive,
                    detail: format!("input width {width} < kernel {kernel_w} with valid padding"),
                });
            }
            ((width - kernel_w) / stride + 1, 0)
        }
    };

    let total = batch * out_w * c_out;
    let mut elements = Vec::with_capacity(total);

    for n in 0..batch {
        for w in 0..out_w {
            for co in 0..c_out {
                let mut acc = 0.0_f64;
                for k in 0..kernel_w {
                    let in_pos = (w * stride + k) as isize - pad_left as isize;
                    if in_pos >= 0 && (in_pos as usize) < width {
                        for ci in 0..c_in {
                            let lhs_idx = n * width * c_in + (in_pos as usize) * c_in + ci;
                            let rhs_idx = k * c_in * c_out + ci * c_out + co;
                            let lhs_val = lhs.elements[lhs_idx].as_f64().unwrap_or(0.0);
                            let rhs_val = rhs.elements[rhs_idx].as_f64().unwrap_or(0.0);
                            acc += lhs_val * rhs_val;
                        }
                    }
                }
                elements.push(Literal::from_f64(acc));
            }
        }
    }

    Ok(Value::Tensor(TensorValue::new(
        DType::F64,
        Shape {
            dims: vec![batch as u32, out_w as u32, c_out as u32],
        },
        elements,
    )?))
}

/// 2D convolution: lhs=[N, H, W, C_in], rhs=[KH, KW, C_in, C_out]
fn eval_conv_2d(
    primitive: Primitive,
    lhs: &TensorValue,
    rhs: &TensorValue,
    params: &BTreeMap<String, String>,
    padding_mode: &str,
) -> Result<Value, EvalError> {
    if rhs.shape.rank() != 4 {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!(
                "2D conv kernel must have rank 4 [KH, KW, C_in, C_out], got rank {}",
                rhs.shape.rank()
            ),
        });
    }

    let batch = lhs.shape.dims[0] as usize;
    let height = lhs.shape.dims[1] as usize;
    let width = lhs.shape.dims[2] as usize;
    let c_in = lhs.shape.dims[3] as usize;

    let kernel_h = rhs.shape.dims[0] as usize;
    let kernel_w = rhs.shape.dims[1] as usize;
    let rhs_c_in = rhs.shape.dims[2] as usize;
    let c_out = rhs.shape.dims[3] as usize;

    if c_in != rhs_c_in {
        return Err(EvalError::Unsupported {
            primitive,
            detail: format!("channel mismatch: lhs c_in={c_in}, rhs c_in={rhs_c_in}"),
        });
    }

    // Parse strides: either single value or "h,w" pair
    let (stride_h, stride_w) = parse_stride_pair(params);

    let (out_h, pad_top) = compute_output_and_pad(height, kernel_h, stride_h, padding_mode);
    let (out_w, pad_left) = compute_output_and_pad(width, kernel_w, stride_w, padding_mode);

    if padding_mode != "same" && padding_mode != "SAME" {
        if height < kernel_h {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!("input height {height} < kernel {kernel_h} with valid padding"),
            });
        }
        if width < kernel_w {
            return Err(EvalError::Unsupported {
                primitive,
                detail: format!("input width {width} < kernel {kernel_w} with valid padding"),
            });
        }
    }

    let total = batch * out_h * out_w * c_out;
    let mut elements = Vec::with_capacity(total);

    for n in 0..batch {
        for oh in 0..out_h {
            for ow in 0..out_w {
                for co in 0..c_out {
                    let mut acc = 0.0_f64;
                    for kh in 0..kernel_h {
                        let in_h = (oh * stride_h + kh) as isize - pad_top as isize;
                        if in_h < 0 || (in_h as usize) >= height {
                            continue;
                        }
                        for kw in 0..kernel_w {
                            let in_w = (ow * stride_w + kw) as isize - pad_left as isize;
                            if in_w < 0 || (in_w as usize) >= width {
                                continue;
                            }
                            for ci in 0..c_in {
                                let lhs_idx = n * height * width * c_in
                                    + (in_h as usize) * width * c_in
                                    + (in_w as usize) * c_in
                                    + ci;
                                let rhs_idx = kh * kernel_w * c_in * c_out
                                    + kw * c_in * c_out
                                    + ci * c_out
                                    + co;
                                let lhs_val = lhs.elements[lhs_idx].as_f64().unwrap_or(0.0);
                                let rhs_val = rhs.elements[rhs_idx].as_f64().unwrap_or(0.0);
                                acc += lhs_val * rhs_val;
                            }
                        }
                    }
                    elements.push(Literal::from_f64(acc));
                }
            }
        }
    }

    Ok(Value::Tensor(TensorValue::new(
        DType::F64,
        Shape {
            dims: vec![batch as u32, out_h as u32, out_w as u32, c_out as u32],
        },
        elements,
    )?))
}

fn parse_stride_pair(params: &BTreeMap<String, String>) -> (usize, usize) {
    let strides_str = params.get("strides").map(String::as_str).unwrap_or("1");
    let parts: Vec<&str> = strides_str.split(',').collect();
    if parts.len() >= 2 {
        let sh = parts[0].trim().parse().unwrap_or(1);
        let sw = parts[1].trim().parse().unwrap_or(1);
        (sh, sw)
    } else {
        let s = parts[0].trim().parse().unwrap_or(1);
        (s, s)
    }
}

fn compute_output_and_pad(
    input_size: usize,
    kernel_size: usize,
    stride: usize,
    padding_mode: &str,
) -> (usize, usize) {
    match padding_mode {
        "same" | "SAME" => {
            let out = input_size.div_ceil(stride);
            let pad_total = ((out - 1) * stride + kernel_size).saturating_sub(input_size);
            (out, pad_total / 2)
        }
        _ => {
            if input_size < kernel_size {
                (0, 0)
            } else {
                ((input_size - kernel_size) / stride + 1, 0)
            }
        }
    }
}
